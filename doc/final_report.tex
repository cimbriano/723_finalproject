\documentclass[11pt, letterpaper, oneside]{article}
\usepackage{enumerate}
\usepackage{ calc }
\usepackage{ amssymb }
\usepackage{ tipa }

\usepackage[margin=1in]{geometry}


\begin{document}

\title{CMSC 723: Final Project \\ Automatically Distinguishing American and British English \\ using Phonetic Transcriptions}
\author{Christopher Imbriano, David Wasser}

\maketitle

%Proposal Questions:
%* what problem are you tackling
%* why is it important/interesting
%* what is your basic approach
%* where will you get data
%* how will you evaluate success
%* what you hope to get out of this project

% 4 pages length

% explain why it works (when it does)
% why it doesn't work (when it doesn't).

%\section{Abstract}
%
%NOTE:  Much of this is placeholder from the progress report and uses future tense.  Change to past tense.

\section{Problem}
	Voice control and command recognition is a popular new feature of smartphones. 
	Apple's Siri and Google's Search App are two examples of systems that take voice commands from users and perform actions like searching the web, creating calendar events, or sending messages.
	Users have been reporting that these systems have trouble understanding them, especially those with British or Southern American accents. 
	Improvements in dialect recognition could help systems like Siri better transcribe user commands even with variations in their speech, as well as improve the localization of the voice the system produces in response.
	
	Solving this entire problem is a complicated engineering and computer science challenge.
	First a system would need to convert speech spoken by individuals into some digital representation.
	Then the digital representation should be transcribed into either text or phonetics.
	Then the text or phonetics would need to be both interpreted for dialectal cues as well as symantically parsed to understand the intent behind the command.
	This project is an attempt to solve just the dialectal portion of the third part of the pipeline. 
	We sought to construct a system that given a phonetic transcription of speech, could distinguish between British and American English.

\section{Data Sources}
	\begin{enumerate}		
		\item Oxford English Dictionary \cite{oed} \\ http://public.oed.com/subscriber-services/sru-service
		
		They have a subscriber service which could be used to compile a corpus of words and their British-English pronunciation.
		
		\item Speech Accent Archive \cite{saa} \\ http://accent.gmu.edu
		
		"The speech accent archive uniformly presents a large set of speech samples from a variety of language backgrounds. 
		Native and non-native speakers of English read the same paragraph and are carefully transcribed. "
		
		\item Not used: CMU Pronunciation Dictionary \cite{cmu} \\ http://www.speech.cs.cmu.edu/cgi-bin/cmudict
		
		"A machine-readable pronunciation dictionary for North American English that contains over 125,000 words and their transcriptions."
		
		This source was not used because it would have required translating a third transcription character set and the approach we decided to pursue did not require another data source. 
		
	\end{enumerate}

\section{ Approach }
	Using a corpus of words for which we have both the American and British phonetic transcription, we built a Probabilistic Finite State Acceptor to serve as a language model for each dialect.
    We used a bigram language model over phonemes since our assumption was that phonetic transcriptions would be input to our distinguisher, and pragmatically, we could use some of the CMSC 723 Project 1: Morphological Analysis \cite{hal} infrastructure and leverage our experience with the Carmel Finite State Machine \cite{carmel}.
	Once we have these two language models, we can feed it phonetic transcriptions of a spoken paragraph.
	The Carmel library outputs a probability for each word in the paragraph for both language models.
	Then a judgement is reached by comparing the number of words determined to be British versus American English, that is, for a given transcription which language model gave the word a higher probability.


\section{ Evaluation Method }

We used the Speech Accent Archive as our test data set for the distinguisher \cite{saa}.
    The Speech Accent Archive, produced by researchers at George Mason University, is a large data set of speakers of various languages speaking a short paragraph along with a phonetic transcription of their speech.
    Included in the set of speakers are individuals from USA and the UK.
    We took a subset of the English speakers from the USA and English speakers from the UK to use as test data.
    This subset had 9 samples of speakers from the UK and 77 samples of speakers from the USA.
    We took this test data and fed it into the distinguisher.
    The distinguisher tried to determine which paragraphs were spoken by American speakers and which were spoken by British speakers.
    To do this, each word was fed into the Finite State Acceptor for both dialects of English to find the probability that this string was generated from that source.
    An overall score was increased by one if the word had a higher probability for the American source than the British source, and decreased by one if the word had a higher probability for the British source.
    At the end of the paragraph, a positive score meant that the paragraph was more likely American English and a negative score meant that the paragraph was more likely British English.
    We then found the percentage of paragraphs that the distinguisher correctly identified and used this as our accuracy measure.

\section{Complications}
	
	\subsection{Character Sets}
	    A number of problems arose from parsing the character sets used by the different data sources.
	    The Speech Accent Archive was in the Rich Text Format using unicode characters to encode the phonetic symbols.
	    The unicode characters were encoded in the format: \textbackslash u\#\#\# where the \# represents decimal digits.
	    To deal with this, we read in the decimal digits and converted them to the appropriate unicode characters that were represented and stored them in UTF-8.
        The Oxford English Dictionary on the other hand encoded the unicode phonetic symbols using UTF-8 \cite{oed}.
	    This caused problems initially when we treated the data as single-byte characters in the scripts preparing the data.
	    We now treat the Oxford English Dictionary data as multi-byte characters when copying individual characters, and as single-byte characters otherwise.
	    In our Finite State Acceptor implementation, all characters are treated as single-byte characters, which should result in equivalent probabilities overall as if interpreted as multi-byte characters.
	
	\subsection{Phonetic Granularity}
	    More problems arose from differences in the specificity of the transcriptions between different datasets.
	    The Oxford English Dictionary encoded primary and secondary stresses in the words, while the Speech Accent Archive did not encode any stresses in the words.
	    To deal with this, we removed all stresses from the data taken from the Oxford English Dictionary.
	    Another issue came from the use of \slash\textteshlig\slash in the Speech Accent Archive as one character while it was represented with two separate unicode characters in the Oxford English Dictionary (using \slash\textipa{t}\slash and \slash\textipa{S}\slash).
	    To fix this, we changed the Speech Accent Archive data to separate the single character into two characters.
        Similarly, we changed R-colored vowels, which only appeared in the Speech Accent Archive to instead be the vowel followed by an \slash r\slash.
	    
	    Additionally, a number of characters only appeared in one of the two datasets.
	    Most of the characters that appeared only in the Oxford English Dictionary were only in uncommon words originating from other languages such as the character y appearing in Saumur \slash somyr\slash, a French word.
	    Many of the characters in the Speech Accent Archive that did not appear in the Oxford English Dictionary resulted from a narrower transcription of the spoken sounds than in the Oxford English Dictionary such as the use of \slash\textipa{R}\slash in the transcription of the word butter.
	    To deal with these, we translated them in the data to similar characters used in the Oxford English Dictionary.

	\subsection{Disparate data sources for testing and model building}
	    We think that a large part of the inaccuracy of the model stems from the differences between the Oxford English Dictionary pronunciations and the Speech Accent Archive pronunciations.
	    The Oxford English Dictionary uses the "proper" pronunciations, which assumes that there is one single correct way to pronounce words in British and American English.
	    This is generally regarded by linguists to not be true and is not the case in the Speech Accent Archive transcriptions.
	    The Speech Accent Archive speech was transcribed the way people actually produced the words instead of in the "proper" manner.
	    As a result, the transcriptions, particularly of the American speakers, generally differ from what the Oxford English Dictionary describes as how to pronounce different words.

	
\section{ Results }
	
	Our distinguisher does not work well.
	We only have one test data set consisting of 86 examples of a transcribed paragraph - 77 speakers of American origin and 9 speakers of British origin.
	Our distinguisher correctly identifies 11 of 86 example paragraphs scoring about 12.8\% accuracy on this dataset.
	This corresponds to correctly identifying all British examples and 2 American examples.
	In other words, the distinguisher is very biased towards British and thus attempts to label nearly all examples as British.
	The low accuracy is an artifact of the skewed distribution of British versus American samples.
	Thus given more test data sets with a roughly even distribution of British versus American, we'd expect to perform no better or worse than chance.
	We would have found an 89.5\% accuracy achieved by determining all samples to be American English just as bad.
	
	The reason for the strong bias towards British hasn't been solidly identified, but we have indications of what might be the cause.
	The language models for both dialects were produced from transcriptions obtained from the Oxford English Dictionary.
	This is not to suggest that the OED is biased towards their mother tongue, but it may be the case that British speakers produce phonetics closer to the "accepted standard" the OED subscribes to.
	 Furthermore, perhaps it is the case that American English speakers have a wider variety of pronunciations, further from the standard.
	 If either or both of these effects were present, it would lead to higher scores for British pronunciations and lower scores for American pronunciations - both would result in poor performance akin to what were are seeing.
	 
	 Another possible cause for error could be the character normalization we had to perform such that the two language models and the test data all transcribed using the same set of characters.
	 The most significant changes were removing diacritical characters from the Speech Accent Archive data (SAA), mapping SAA-only characters to OED characters and removing stress markers from OED data.
	 The reasoning behind this was that stress and diacritical characters augment the phonemes rather than change them significantly, so for example where a word begins with a stress like mother (brit.) \slash\textipa{"m2D@}\slash we simply dropped the stress character to make \slash\textipa{m2D@}\slash.
	 While this change seems reasonable, perhaps the following character mappings made American English speech appear more British:
	 
	 \begin{table}[h]
	 \centering
	 	\begin{tabular}{c | c}
		SAA-only & OED Mapping\\ \hline
		
		\textrhookrevepsilon & \textipa{3r} \\
		\textrhookschwa & \textipa{@r} \\
		\textteshlig & \textipa{tS} \\
		\textipa{\:t} & \textipa{t} \\
		\textipa{B} & \textipa{b} \\
		\textturna & \textipa{\ae} \\
		\textipa{T} & \textipa{d} \\
		\textipa{\*r} & \textipa{r} \\
		\textipa{R} & \textipa{d} \\
		\textipa{P} & \textipa{t} \\
		\textipa{\:R} & \textipa{r} \\
		\textcloseepsilon & \textipa{3} \\
		\textreve & \textipa{@} \\
		\textipa{1} & \textipa{i:} \\
		\textipa{0} & \textipa{u:} \\
		
		\end{tabular}
	 \caption{Mapping from SAA-only character to OED "equivalent"}
	 \label{tab:mapping}
	 \end{table}
	 
\section{Future Work}

	Given the opportunity to continue working on this project, we have another approach that we would try.
    Instead of using the OED transcriptions to build both the American and British language model, we could have used the CMU Dictionary as the data source for the American English model \cite{cmu}.
	In this scenario, we'd have to translate the ARPAbet, the transcription alphabet used by the CMU dictionary into the character set used by the OED, which shouldn't be too difficult given that there are only 39 phonemes in the ARPAbet.
	To evaluate, we could select a subset of words from both the CMU dictionary and the OED and have the language models pick the more probable dialect.
	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{9}
	\bibitem{oed} \textit{Oxford English Dictionary} Retrieved December 8, 2012 from http://www.oed.com
    \bibitem{saa} Weinberger, S. (2012, December, 9). \textit{The Speech Accent Archive}. Retrieved December 18, 2012 from http://accent.gmu.edu/index.php
    \bibitem{cmu} \textit{The CMU Pronouncing Dictionary} Retrieved December 1, 2012 from http://www.speech.cs.cmu.edu/cgi-bin/cmudict
    \bibitem{hal} Daumé III, H. \textit{CS 723 Project 1: Morphological Disambiguation} [Computer program]. Retrieved December 15, 2012 from http://www.umiacs.umd.edu/~hal/courses/2012F\_CL1/p1/
    \bibitem{carmel} Graehl J. \textit{Carmel Finite-State Toolkit} (Version 6.12) [Computer program]. Retrieved December 18, 2012, from http://www.isi.edu/licensed-sw/carmel/
\end{thebibliography}

\end{document}
