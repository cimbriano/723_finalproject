\documentclass[11pt, letterpaper, oneside]{article}
\usepackage{enumerate}
\usepackage{ calc }
\usepackage{ amssymb }

\begin{document}

\title{CMSC 723: Final Project \\ Automatically Distinguishing American and British English}
\author{Christopher Imbriano, David Wasser}

\maketitle

%Proposal Questions:
%* what problem are you tackling
%* why is it important/interesting
%* what is your basic approach
%* where will you get data
%* how will you evaluate success
%* what you hope to get out of this project

% 4 pages length

% explain why it works (when it does)
% why it doesn't work (when it doesn't).

% You should also reserve about a page of this writeup to talk about what _didn't_ work and what was harder than you expected it to be (there _will_ be something!).
\section{Abstract}

NOTE:  Much of this is placeholder from the progress report and uses future tense.  Change to past tense.

\section{Problem}
Voice control and command recognition is a popular new feature of smartphones. 
Apple's Siri and Google's Search App are two examples of systems that take voice commands form users and performs actions like searching the web, creating calendar events, or sending messages.
Users had been reporting that these systems have been having trouble understanding them, especially those with British or Southern American accents. 
Improvements in accent understanding could help systems like these better recognize users with variations in their speech, as well as improve the localization of the voice the system produces.

\section{Introduction}


\section{Data Sources}
	\begin{enumerate}
		\item CMU Pronunciation Dictionary \\ http://www.speech.cs.cmu.edu/cgi-bin/cmudict
		
		"A machine-readable pronunciation dictionary for North American English that contains over 125,000 words and their transcriptions."
		
		\item Oxford English Dictionary \\ http://public.oed.com/subscriber-services/sru-service
		
		They have a subscriber service which could be used to compile a corpus of words and their British-English pronunciation.
		
		\item Speech Accent Archive \\ http://accent.gmu.edu
		
		"The speech accent archive uniformly presents a large set of speech samples from a variety of language backgrounds. 
		Native and non-native speakers of English read the same paragraph and are carefully transcribed. "
	\end{enumerate}

\section{ Method }
Using a corpus of words for which we have both the American and British phonetic transcription, we will build a Probabilistic Finite State Acceptor.
We will build a language model over phonemes since we will assume phonetic transcriptions are our input data.
Once we have these two language models, one for each dialect, we can feed it phonetic transcriptions and receive a probability of that transcription in each of British and American English.
The distinguisher will produce a judgment based on the more probable dialect.


\section{ Results }

	\subsection{ Evaluation Method }
	This paragraph is from the progress report - needs updating.
	
	The Speech Accent Archive, produced by researchers at George Mason University, it a large data set of speakers of various languages speaking a sentence as well as the phonetic transcription of their speech.
	Included in the set of speakers are individuals from USA and the UK.
	We will use these two subsets as ground truth. So given the transcribed speech of speakers in those sets, we expect that our distinguisher should correctly identify each speaker's native dialect.
	Evaluation will then be a percentage of speakers correctly identified.
	

	

\section{Complications}

\begin{itemize}
\item Character sets
    A number of problems arose from parsing the character sets used by the different data sources.
    The Speech Accent Archive was in the Rich Text Format using unicode characters to encode the phonetic symbols.
    The unicode characters were encoded in the format: \textbackslash u### where the # represents decimal digits.
    To deal with this, we read in the decimal digits and converted them to the appropriate unicode characters that were represented and stored them in UTF-8.
    The Oxford English Dictionary on the other hand encoded the unicode phonetic symbols using UTF-8.
    This caused problems initially when we treated the data as single-byte characters in the scripts preparing the data.
    We now treat the Oxford English Dictionary data as multi-byte characters when copying individual characters, and as single-byte characters otherwise.
    In our Finite State Acceptor implementation, all characters are treated as single-byte characters, which should result in equivalent probabilities overall as if interpreted as multi-byte characters.

\item Phonetic Granularity
    More problems arose from differences in the specificity of the transcriptions between different datasets.
    The Oxford English Dictionary encoded primary and secondary stresses in the words, while the Speech Accent Archive did not encode any stresses in the words.
    To deal with this, we removed all stresses from the data taken from the Oxford English Dictionary.
    Another issue came from the use of ʧ in the Speech Accent Archive as one character while it was represented with two separate unicode characters in the Oxford English Dictionary (using t and ʃ).
    To fix this, we changed the Speech Accent Archive data to separate the single character into two characters.
    A number of characters also only appeared in one of the two datasets.
    Most of the characters that appeared only in the Oxford English Dictionary were only in uncommon words originating from other languages such as the character y appearing in Saumur, a French word.
    Many of the characters in the Speech Accent Archive that did not appear in the Oxford English Dictionary resulted from a narrower transcription of the spoken sounds than in the Oxford English Dictionary such as the use of ɾ in the transcription of the word butter.
    To deal with these, we translated them in the data to similar characters used in the Oxford English Dictionary.

\item Disparate data sources for testing and model building
    We think that a large part of the innacuracy of the model stems from the differences between the Oxford English Dictionary pronunciations and the Speech Accent Archive pronunciations.
    The Oxford English Dictionary uses the "proper" pronunciations, which assumes that there is one single correct way to pronounce words in British and American English.
    This is generally regarded by linguists to not be true and is not the case in the Speech Accent Archive transcriptions.
    The Speech Accent Archive transcriptions are transcribed the way people actually spoke the words instead of in the "proper" manner.
    As a result, the transcriptions, particularly of the American speakers, generally differ from what the Oxford English Dictionary describes as how to pronounce different words.

\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{9}
%	\bibitem{sads} Elaine Shi, Charalampos Papamanthou \emph{Streaming Authenticated Data Structures}, Date Unknown
\end{thebibliography}

\end{document}
